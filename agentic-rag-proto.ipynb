{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6902b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict, Union, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf00e3b",
   "metadata": {},
   "source": [
    "# 1. Document preparation\n",
    "\n",
    "This section is about the document processing in the application. I use a specialized loader to extract text from PDF documents located in the `pdf_docs` directory. The documents are then split into manageable chunks using the `RecursiveCharacterTextSplitter`, which creates semantic units of text with appropriate overlap to maintain context between chunks.\n",
    "\n",
    "The `prepare_documents` function handles the entire document preparation workflow:\n",
    "1. Loading PDFs from the specified directory\n",
    "2. Extracting text content while handling errors behind the scenes\n",
    "3. Splitting documents into smaller chunks (1000 characters with 200 character overlap)\n",
    "4. Adding source metadata and position tracking for better reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962919ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_documents(docs_folder: str = \"./pdf_docs\") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load and split PDF documents from a specified directory.\n",
    "\n",
    "    Args:\n",
    "        docs_folder (str, optional): The file containing folder. Defaults to \"./pdf_docs\".\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document objects containing the text from the PDFs split into chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFDirectoryLoader(\n",
    "        path=docs_folder,\n",
    "        glob=\"**/*.pdf\",\n",
    "        silent_errors=True\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039a2f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = prepare_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a939866",
   "metadata": {},
   "source": [
    "# 2. Vector store creation\n",
    "\n",
    "This section focuses on creating a vector store to enable semantic search capabilities in my application. After processing and splitting the PDF documents into chunks, I need to convert them into vector representations for efficient retrieval.\n",
    "\n",
    "The process involves:\n",
    "\n",
    "1. Initializing a Hugging Face embedding model (`sentence-transformers/all-MiniLM-L6-v2`) that converts text into numerical vector representations\n",
    "2. Creating a Chroma vector database from the document chunks using the embedding model\n",
    "3. Persisting the vector store to disk for future use without reprocessing documents\n",
    "\n",
    "The vector store serves as the foundation for my retrieval-augmented generation system, allowing the application to find the most relevant document chunks based on semantic similarity rather than simple keyword matching.\n",
    "\n",
    "Reason of Hugging Face embedding:\n",
    "- The Hugging Face embedding model is lightweight and efficient, making it suitable for real-time applications.\n",
    "- It provides high-quality embeddings that capture semantic meaning, enabling better retrieval of relevant information.\n",
    "\n",
    "Reason of Chroma vector database:\n",
    "- Chroma is designed for fast and efficient vector storage and retrieval, making it ideal for applications that require quick access to large amounts of vector data.\n",
    "- It supports various indexing and querying methods, allowing for flexible and scalable vector search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2838679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006660f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_vector_store(documents: List[Document], embedding_model: Embeddings, persist_dir: str = \"./chroma_db\") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create a Chroma vector store from the provided documents and embedding model.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of Document objects to be stored in the vector store.\n",
    "        embedding_model (Embeddings): The embedding model to be used for vectorization.\n",
    "        dir (str, optional): Directory to persist the collection. Defaults to \"./chroma_db\".\n",
    "\n",
    "    Returns:\n",
    "        Chroma: _description_\n",
    "    \"\"\"\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_dir,\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f108bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_chroma_vector_store(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b1087",
   "metadata": {},
   "source": [
    "# 3. Agent state definition\n",
    "\n",
    "This section defines the state management for the Retrieval-Augmented Generation (RAG) workflow. The `AgentState` uses a TypedDict to provide a structured representation of the agent's state throughout the execution process.\n",
    "\n",
    "The state contains:\n",
    "\n",
    "- **messages**: Conversation history between the user and AI\n",
    "- **context**: Additional information or extracted context relevant to the query\n",
    "- **query**: The user's original question or request\n",
    "- **retrieval_strategy**: Method used to retrieve documents (e.g., \"multi_step\", \"deep_analysis\", \"direct\")\n",
    "- **next_nodes**: List of nodes to be executed in the workflow\n",
    "- **retrieved_docs**: Documents fetched from the vector store based on the query\n",
    "- **response**: The generated answer to the user's query\n",
    "- **needs_correction**: Flag indicating if the response needs revision\n",
    "- **reflection_feedback**: Feedback from verification processes\n",
    "\n",
    "This state-based approach enables the workflow to maintain context across different processing nodes and make decisions based on accumulated information, creating a more coherent and informed interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e0faa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    AgentState represents the state of the agent in the state graph.\n",
    "    \"\"\"\n",
    "    messages: List[Union[HumanMessage, AIMessage]]\n",
    "    context: Optional[str]\n",
    "    query: str\n",
    "    retrieval_strategy: Optional[str]\n",
    "    next_nodes: Optional[List[str]]\n",
    "    retrieved_docs: List[str]\n",
    "    response: Optional[str]\n",
    "    needs_correction: Optional[bool]\n",
    "    reflection_feedback: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc43089",
   "metadata": {},
   "source": [
    "# 4. LLM model definition\n",
    "\n",
    "In this section, I initialize a large language model (LLM) to handle the natural language processing tasks in my RAG application. I'm using the Ollama framework to run a local instance of the Llama3 model, which offers several advantages for this implementation:\n",
    "\n",
    "- **Local deployment**: Running the model locally ensures data privacy and reduces latency\n",
    "- **No API costs**: Eliminates usage fees associated with commercial LLM services\n",
    "- **Customization**: Ability to fine-tune parameters for specific use cases\n",
    "- **Offline operation**: Can function without internet connectivity\n",
    "\n",
    "The Llama3 model provides a good balance between performance and resource requirements, making it suitable for deployment on consumer hardware while still delivering high-quality responses for document-based question answering.\n",
    "\n",
    "This LLM powers various components of the workflow, including query analysis, document retrieval optimization, response generation, and fact-checking processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c68f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de666f2c",
   "metadata": {},
   "source": [
    "# 5. Node implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4c038",
   "metadata": {},
   "source": [
    "### Orchestrator node\n",
    "\n",
    "The Orchestrator node is the central decision-making component in the RAG workflow. It analyzes the user's query to determine the most appropriate processing path based on the query's nature and complexity.\n",
    "\n",
    "Key functions of the Orchestrator:\n",
    "\n",
    "- Query analysis: Examines the question to classify it into different categories (factual information search, complex analysis, or simple question)\n",
    "- Workflow routing: Selects the optimal processing flow based on the query classification\n",
    "- Strategy assignment: Sets the retrieval strategy to guide how documents are fetched and processed\n",
    "- Context enhancement: Adds query classification information to the state context\n",
    "\n",
    "The Orchestrator implements adaptive pathways, ensuring that simple questions receive direct answers while complex queries trigger deeper information retrieval and analysis. This intelligence-driven approach optimizes both processing efficiency and response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f674014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Orchestrator function to determine the next steps based on the user's query.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the next nodes to process.\n",
    "    \"\"\"\n",
    "\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    analysis_prompt = f\"\"\"\n",
    "    Analyze the following user question and determine the appropriate processing flow:\n",
    "\n",
    "    QUESTION: {query}\n",
    "\n",
    "    Choose from the following options:\n",
    "    1. Factual information search\n",
    "    2. Complex analysis\n",
    "    3. Simple question\n",
    "    \"\"\"\n",
    "\n",
    "    analysis_result = llm.invoke(analysis_prompt)\n",
    "\n",
    "    if \"1\" in analysis_result:\n",
    "        state[\"retrieval_strategy\"] = \"multi_step\"\n",
    "        state[\"next_nodes\"] = [\"keyword_analyzer\", \"document_retriever\"]\n",
    "    elif \"2\" in analysis_result:\n",
    "        state[\"retrieval_strategy\"] = \"deep_analysis\"\n",
    "        state[\"next_nodes\"] = [\"keyword_analyzer\", \"document_retriever\", \"summarizer\"]\n",
    "    else:\n",
    "        state[\"retrieval_strategy\"] = \"direct\"\n",
    "        state[\"next_nodes\"] = [\"response_generator\"]\n",
    "\n",
    "    state[\"context\"] = f\"Type of the question: {analysis_result.strip()}\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd922bc",
   "metadata": {},
   "source": [
    "### Document retriever node\n",
    "\n",
    "The Document Retriever node is responsible for querying the vector database to find the most relevant document chunks based on the user's query. This critical component implements different retrieval strategies based on the query type identified by the Orchestrator:\n",
    "\n",
    "- **Multi-step retrieval**: Uses Maximum Marginal Relevance (MMR) search to find diverse but relevant results, balancing similarity with information diversity\n",
    "- **Deep analysis retrieval**: Expands the original query using the LLM before searching, retrieving more comprehensive results for complex questions\n",
    "- **Direct retrieval**: Uses standard similarity search for straightforward questions that need concise answers\n",
    "\n",
    "The node processes the retrieved documents by extracting metadata (source file and page number) and formatting the content for easy reference in subsequent processing steps. This approach ensures that responses can be properly attributed to their sources, maintaining traceability and allowing for fact verification.\n",
    "\n",
    "By adapting the retrieval method to the query type, the system optimizes both search relevance and computational efficiency, retrieving just enough information to answer the question accurately without overwhelming later processing steps with irrelevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d785f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_retriever(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Document retrieval function to fetch relevant documents based on the user's query.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the retrieved documents.\n",
    "    \"\"\"\n",
    "\n",
    "    query = state[\"query\"]\n",
    "    strategy = state[\"retrieval_strategy\"]\n",
    "\n",
    "    if strategy == \"multi_step\":\n",
    "        docs = vector_store.max_marginal_relevance_search(query, k=5, fetch_k=20)\n",
    "    elif strategy == \"deep_analysis\":\n",
    "        expanded_query = llm.invoke(f\"Expand the search query: {query}\")\n",
    "        docs = vector_store.similarity_search(expanded_query, k=7)\n",
    "    else:\n",
    "        docs = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "    processed_docs = []\n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata\n",
    "        source = metadata.get(\"source\", \"Unknown source\")\n",
    "        page = metadata.get(\"page\", \"N/A\")\n",
    "        processed_docs.append(\n",
    "            f\"[{Path(source).name} - page {page}]\\n{doc.page_content}\"\n",
    "        )\n",
    "\n",
    "    state[\"retrieved_docs\"] = processed_docs\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e87c1c",
   "metadata": {},
   "source": [
    "### Fact checker node\n",
    "\n",
    "The Fact Checker node is responsible for verifying the accuracy of generated responses against the retrieved document evidence. This critical validation step ensures that the information provided to users is factually correct and properly supported by source materials.\n",
    "\n",
    "Key functions of the Fact Checker:\n",
    "\n",
    "- Systematic verification: Compares each statement in the response against information in the retrieved documents\n",
    "- Contradiction detection: Identifies claims that conflict with source materials or lack supporting evidence\n",
    "- Quantitative assessment: Provides a numerical accuracy score to measure response quality\n",
    "- Correction guidance: When inaccuracies are found, highlights specific errors and suggests improvements\n",
    "\n",
    "The node uses a structured prompt that guides the LLM to methodically evaluate response accuracy, looking for supported statements, contradictions, and information gaps. This approach creates a feedback loop that improves response quality and maintains factual integrity throughout the RAG workflow.\n",
    "\n",
    "By implementing this verification step before delivering responses to users, the system significantly reduces the risk of hallucinations and factual errors that can undermine trust in AI-generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a11c629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_checker(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Fact-checking function to verify the accuracy of the generated response based on retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the verification results.\n",
    "    \"\"\"\n",
    "\n",
    "    response = state[\"response\"]\n",
    "    docs = state[\"retrieved_docs\"]\n",
    "\n",
    "    verification_prompt = f\"\"\"\n",
    "    Check the accuracy of the following answer based on the retrieved documents:\n",
    "\n",
    "    ANSWER: {response}\n",
    "\n",
    "    DOCUMENTS: {docs}\n",
    "\n",
    "    Identify:\n",
    "    1. Statements supported by the documents\n",
    "    2. Contradictory parts of the documents\n",
    "    3. Missing information\n",
    "\n",
    "    Format:\n",
    "    - Accuracy level: X%\n",
    "    - Incorrect items: [list]\n",
    "    - Suggestions: [list]\n",
    "    \"\"\"\n",
    "\n",
    "    verification_result = llm.invoke(verification_prompt)\n",
    "\n",
    "    if \"Incorrect items:\" in verification_result and len(verification_result.split(\"Incorrect items:\")) > 1:\n",
    "        errors = verification_result.split(\"Incorrect items:\")[1].split(\"- Suggestions:\")[0]\n",
    "        state[\"needs_correction\"] = True\n",
    "        state[\"reflection_feedback\"] = f\"Errors found: {errors}\"\n",
    "    else:\n",
    "        state[\"needs_correction\"] = False\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0ed2b",
   "metadata": {},
   "source": [
    "### Keyword analyzer node\n",
    "\n",
    "The Keyword Analyzer node extracts key search terms and contextual information from user queries to enhance retrieval accuracy. It serves as a preprocessing step that helps narrow down and focus document searches by identifying:\n",
    "\n",
    "- Main topics and concepts relevant to the query\n",
    "- Potential search operators for more targeted retrieval\n",
    "- Time-based constraints or references that might limit the search scope\n",
    "\n",
    "This analysis provides additional context that improves downstream processing by:\n",
    "\n",
    "1. Enriching the query with semantic dimensions beyond simple keyword matching\n",
    "2. Identifying specialized terminology that might require focused attention\n",
    "3. Determining temporal aspects that might affect document relevance\n",
    "\n",
    "By breaking down complex queries into their fundamental components, the keyword analyzer enables more precise document retrieval and helps optimize the use of computational resources by focusing searches on the most relevant content areas.\n",
    "\n",
    "The structured output from this node becomes part of the state context, guiding subsequent retrieval operations and ensuring that document searches align closely with the user's informational needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7ec5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_analyzer(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Keyword analysis function to extract keywords and search contexts from the user's query.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the extracted keywords and contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    extracion_prompt = f\"\"\"\n",
    "    Break down the following question into keywords and search contexts:\n",
    "\n",
    "    QUESTION: {query}\n",
    "\n",
    "    Format:\n",
    "    - Main topics: [comma separated]\n",
    "    - Search operators: [site:, filetype:, etc.]\n",
    "    - Time limits: [years, dates]\n",
    "    \"\"\"\n",
    "\n",
    "    extraction_result = llm.invoke(extracion_prompt)\n",
    "    state[\"context\"] = extraction_result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5fa37",
   "metadata": {},
   "source": [
    "### Summarizer node\n",
    "\n",
    "The Summarizer node creates concise summaries of retrieved document chunks, serving as an intermediate processing step for complex queries. This component streamlines vast amounts of information into digestible formats that:\n",
    "\n",
    "1. **Highlight key information**: Extracts the most relevant facts and concepts from document chunks\n",
    "2. **Reduce cognitive load**: Condenses lengthy text into essential points for easier comprehension\n",
    "3. **Maintain source attribution**: Preserves document origins for traceability and verification\n",
    "\n",
    "When the Orchestrator determines that a query requires deep analysis, the summarization process runs after document retrieval but before response generation. This approach helps manage information complexity by:\n",
    "\n",
    "- Synthesizing multiple document perspectives into a coherent narrative\n",
    "- Eliminating redundant information across document chunks\n",
    "- Preserving crucial details while reducing overall volume\n",
    "\n",
    "The summarizer uses a structured prompt template that instructs the LLM to create bullet-point summaries with source citations, ensuring that information remains attributable and verifiable. By functioning as an information filter, this node significantly improves response quality for complex analytical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fedb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Summarization function to create a concise summary of the retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the generated summary.\n",
    "    \"\"\"\n",
    "\n",
    "    docs = state[\"retrieved_docs\"]\n",
    "    \n",
    "    summary_prompt = f\"\"\"\n",
    "    Summarize the following document excerpts, highlighting the most important information:\n",
    "\n",
    "    {\"\".join(docs)}\n",
    "\n",
    "    Important:\n",
    "    - Maximum 5 sentences\n",
    "    - Use bullet points (but only as text)\n",
    "    - Cite sources: [Document Name - page X]\n",
    "    \"\"\"\n",
    "\n",
    "    state[\"context\"] = llm.invoke(summary_prompt)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512cdb22",
   "metadata": {},
   "source": [
    "### Response generator node\n",
    "\n",
    "The Response Generator node is responsible for creating comprehensive and informative answers to user queries based on the retrieved documents and analyzed context. This component transforms raw document information into coherent, user-friendly responses that directly address the original question.\n",
    "\n",
    "Key functions of the Response Generator:\n",
    "\n",
    "1. **Contextual understanding**: Leverages both the original query and accumulated context to generate relevant responses\n",
    "2. **Structured output**: Creates organized answers with clear sections including summary, detailed analysis, and source references\n",
    "3. **Source attribution**: Maintains academic integrity by citing the specific documents that informed the response\n",
    "4. **Supplementary guidance**: Provides additional suggestions or related topics when appropriate\n",
    "\n",
    "The node uses a structured prompt template that guides the LLM to create consistent, well-formatted responses that balance brevity with comprehensiveness. This approach ensures that users receive clear, actionable information while maintaining transparency about the source of that information.\n",
    "\n",
    "By functioning as the synthesis engine of the RAG workflow, this node translates complex document retrieval and analysis into natural language responses that effectively satisfy user information needs while maintaining factual accuracy and source traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d0fcbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generator(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Response generation function to create a comprehensive answer based on the user's query and context.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    context = state[\"context\"]\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    response_prompt = f\"\"\"\n",
    "    Prepare a comprehensive answer to the following question:\n",
    "\n",
    "    QUESTION: {query}\n",
    "\n",
    "    CONTEXT: {context}\n",
    "\n",
    "    Follow the structure below:\n",
    "    1. Short summary (1 sentence)\n",
    "    2. Detailed analysis (maximum 5 sentences)\n",
    "    3. References: [Document Name - page X]\n",
    "    4. Additional suggestions (if relevant)\n",
    "    \"\"\"\n",
    "\n",
    "    state[\"response\"] = llm.invoke(response_prompt)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65b190",
   "metadata": {},
   "source": [
    "### Reflection node\n",
    "\n",
    "The Reflection node serves as a quality control mechanism that critically evaluates generated responses before they reach the user. This node performs a thorough analysis of the response against the retrieved documents to ensure factual accuracy and consistency.\n",
    "\n",
    "Key functions of the Reflection node:\n",
    "\n",
    "- **Deep verification**: Analyzes the response for factual accuracy by cross-referencing with retrieved documents\n",
    "- **Error identification**: Detects inaccurate statements, contradictions, missing citations, and inconsistencies\n",
    "- **Feedback loop**: Provides detailed analysis when issues are found to guide correction\n",
    "- **Quality assurance**: Acts as a final checkpoint before delivering information to the user\n",
    "\n",
    "The node uses a structured prompt that guides the LLM to systematically evaluate the response across multiple dimensions of accuracy. When errors are detected, the workflow routes the response back to the Response Generator for revision based on the reflection feedback.\n",
    "\n",
    "This self-correcting mechanism significantly improves response quality by catching potential errors that might have escaped earlier verification steps. By implementing this multi-layered verification approach, the system can maintain high standards of accuracy while reducing the likelihood of delivering misleading information to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b41e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Reflection function to analyze the generated response and check for inconsistencies or errors.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the reflection results.\n",
    "    \"\"\"\n",
    "\n",
    "    response = state[\"response\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "\n",
    "    reflection_prompt = f\"\"\"\n",
    "    Analyze the following answer using the retrieved documents:\n",
    "\n",
    "    ANSWER: {response}\n",
    "\n",
    "    DOCUMENTS: {retrieved_docs}\n",
    "\n",
    "    Identify:\n",
    "    - Inaccurate statements\n",
    "    - Contradictory statements\n",
    "    - Missing source references\n",
    "    - Inconsistencies with documents\n",
    "\n",
    "    If the answer is correct, respond with \"allcorrect\". If not, provide a detailed analysis of the issues found.\n",
    "    \"\"\"\n",
    "\n",
    "    reflection_result = llm.invoke(reflection_prompt)\n",
    "    if \"allcorrect\" not in reflection_result:\n",
    "        state[\"needs_correction\"] = True\n",
    "        state[\"reflection_feedback\"] = reflection_result\n",
    "    else:\n",
    "        state[\"needs_correction\"] = False\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022a2fe",
   "metadata": {},
   "source": [
    "# 6. Workflow building\n",
    "\n",
    "This section details the construction of a state-based workflow graph for our Retrieval-Augmented Generation (RAG) system. The workflow orchestrates how information flows between the specialized nodes to create an intelligent document retrieval and question answering system.\n",
    "\n",
    "## Workflow architecture\n",
    "\n",
    "The workflow follows a directed graph structure where:\n",
    "\n",
    "- Each node represents a specialized processing component (orchestrator, retriever, generator, etc.)\n",
    "- Edges determine the flow of information between components\n",
    "- Conditional logic enables dynamic routing based on processing outcomes\n",
    "- State management preserves context across the entire processing pipeline\n",
    "\n",
    "## Key workflow components\n",
    "\n",
    "1. **Core processing path**: Orchestrates the flow from query analysis through document retrieval to response generation\n",
    "2. **Adaptive routing**: Implements conditional paths based on query complexity and verification results\n",
    "3. **Self-correction mechanism**: Creates feedback loops when responses need improvement\n",
    "4. **State preservation**: Maintains context, retrieved documents, and processing history throughout execution\n",
    "\n",
    "## Benefits of graph-based workflow\n",
    "\n",
    "- **Modularity**: Each node can be developed, tested, and optimized independently\n",
    "- **Flexibility**: Easy to modify the workflow by adding, removing, or rerouting nodes\n",
    "- **Visibility**: Clear visualization of information flow and decision points\n",
    "- **Maintainability**: Well-defined interfaces between components simplify updates\n",
    "\n",
    "The graph design pattern enables complex decision-making while maintaining clean separation of concerns between different stages of the RAG process, resulting in more robust and adaptable document-based question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d730ee4",
   "metadata": {},
   "source": [
    "### Init the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3af30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7b55b",
   "metadata": {},
   "source": [
    "### Adding the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b100779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x27e5f999400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"orchestrator\", orchestrator)\n",
    "workflow.add_node(\"keyword_analyzer\", keyword_analyzer)\n",
    "workflow.add_node(\"document_retriever\", document_retriever)\n",
    "workflow.add_node(\"summarizer\", summarizer)\n",
    "workflow.add_node(\"response_generator\", response_generator)\n",
    "workflow.add_node(\"fact_checker\", fact_checker)\n",
    "workflow.add_node(\"reflection\", reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b51cc7",
   "metadata": {},
   "source": [
    "### Adding the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba4301d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x27e5f999400>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(\"orchestrator\", \"keyword_analyzer\")\n",
    "workflow.add_edge(\"keyword_analyzer\", \"document_retriever\")\n",
    "workflow.add_edge(\"document_retriever\", \"summarizer\")\n",
    "workflow.add_edge(\"summarizer\", \"response_generator\")\n",
    "workflow.add_edge(\"response_generator\", \"fact_checker\")\n",
    "workflow.add_edge(\"fact_checker\", \"reflection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd77b2f",
   "metadata": {},
   "source": [
    "### Adding conditional transitions\n",
    "\n",
    "This section implements conditional routing logic in our RAG workflow graph, enabling dynamic decision paths based on processing outcomes. The conditional transitions create intelligent branching that allows the system to:\n",
    "\n",
    "1. **Self-correct responses**: When fact-checking or reflection detects inaccuracies, the workflow routes back to the response generator for revision\n",
    "2. **Optimize processing efficiency**: Only execute necessary nodes based on query complexity and intermediate results\n",
    "3. **Create feedback loops**: Enable iterative improvement through verification-correction cycles\n",
    "\n",
    "The branching logic particularly focuses on the reflection step, where the system determines whether to:\n",
    "- Return to the response generator when corrections are needed\n",
    "- Conclude the workflow when the response meets quality standards\n",
    "\n",
    "This conditional architecture creates a self-improving system that can identify and address potential inaccuracies before delivering final responses to users, significantly enhancing the reliability of AI-generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c6a61ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x27e5f999400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def route_after_reflection(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Routing function to determine the next step after reflection based on the state.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, including the user's query and context.\n",
    "\n",
    "    Returns:\n",
    "        str: The next node to process based on the reflection results.\n",
    "    \"\"\"\n",
    "\n",
    "    return \"response_generator\" if state[\"needs_correction\"] else END\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflection\",\n",
    "    route_after_reflection,\n",
    "    {\n",
    "        \"response_generator\": \"response_generator\",\n",
    "        END: END,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6eeb76",
   "metadata": {},
   "source": [
    "### Defining entry and endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0e714e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x27e5f999400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"orchestrator\")\n",
    "workflow.add_edge(\"reflection\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8798f5",
   "metadata": {},
   "source": [
    "# 7. Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e18c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_rag = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f6fe3",
   "metadata": {},
   "source": [
    "# 8. Starting the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf6422bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent():\n",
    "    while True:\n",
    "        query = input(\"Ask something (to leave type 'exit'): \")\n",
    "        if query.lower().strip() == \"exit\":\n",
    "            break\n",
    "\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": [],\n",
    "            \"context\": None,\n",
    "            \"done\": False,\n",
    "        }\n",
    "\n",
    "        final_state = agentic_rag.invoke(initial_state)\n",
    "\n",
    "        print(\"\\nResponse:\")\n",
    "        print(final_state[\"response\"])\n",
    "        print(\"\\nUsed documents:\")\n",
    "        for doc in final_state[\"retrieved_docs\"]:\n",
    "            print(f\"- {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a06d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "Here is a comprehensive answer to the question \"What cats do around humans\":\n",
      "\n",
      "**Summary:** Domestic cats form social groups and have forms of communication that help them live harmoniously, even in close proximity to humans.\n",
      "\n",
      "**Detailed Analysis:** While it's common to think of cats as solitary animals, research has shown that domestic cats are actually social creatures. They have a natural inclination to form colonies or groups when there is sufficient food resources available, which allows for social interaction and communication. This social behavior is not limited to free-living domestic cats; even those living with humans can exhibit forms of social organization and communication. In fact, many domestic cats choose to live in close proximity to their human caregivers, often seeking out human interaction and affection.\n",
      "\n",
      "**References:** [FelineBehaviorGLS.pdf - page 8]\n",
      "\n",
      "**Additional Suggestions:** For cat owners or those interested in feline behavior, observing and understanding the social dynamics of cats can be fascinating. By recognizing that domestic cats are social animals, we can better appreciate their natural behaviors and respond more effectively to their needs. This might involve providing multiple social outlets for cats, such as introducing them to other felines or engaging them in interactive play with humans.\n",
      "\n",
      "Used documents:\n",
      "- [FelineBehaviorGLS.pdf - page 8]\n",
      "determine whether the behavior is a normal behavior for\n",
      "the cat tha...\n",
      "- [FelineBehaviorGLS.pdf - page 8]\n",
      "determine whether the behavior is a normal behavior for\n",
      "the cat tha...\n",
      "- [FelineBehaviorGLS.pdf - page 35]\n",
      "36\n",
      "22. Kerby G, Macdonald DW. Cat society and the consequences of ...\n",
      "- [FelineBehaviorGLS.pdf - page 35]\n",
      "ology and epidemiology. In: The domestic cat: the biology of its b...\n",
      "- [eg14_cats_and_people.pdf - page 2]\n",
      "after them and in some cases, controlled their breeding. This \n",
      "h...\n"
     ]
    }
   ],
   "source": [
    "chat_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf44b48",
   "metadata": {},
   "source": [
    "# -- How to test and measure the performance of the system --\n",
    "\n",
    "## Performance metrics\n",
    "\n",
    "The evaluation of the RAG system should be based on both quantitative metrics and qualitative assessments:\n",
    "\n",
    "### Quantitative metrics\n",
    "- **Response time**: Measure the time taken from query submission to final response\n",
    "- **Retrieval precision**: Calculate the relevance of retrieved documents to the query\n",
    "- **Retrieval recall**: Assess how many relevant documents were retrieved versus missed\n",
    "- **Answer accuracy**: Compare generated responses against ground truth answers\n",
    "- **Correction rate**: Track how often the system needed to self-correct responses\n",
    "\n",
    "### Qualitative assessments\n",
    "- **Response coherence**: Evaluate the logical flow and organization of responses\n",
    "- **Source attribution**: Check if responses properly cite relevant document sources\n",
    "- **Factual consistency**: Verify that responses align with information in the documents\n",
    "- **Query understanding**: Assess how well the system interprets different question types\n",
    "\n",
    "## Testing methodology\n",
    "\n",
    "### Benchmark testing\n",
    "1. Create a test set of representative queries with known ground truth answers\n",
    "2. Execute each query through the workflow and record performance metrics\n",
    "3. Compare results across different query types (factual, analytical, simple)\n",
    "4. Identify patterns in performance variations based on query complexity\n",
    "\n",
    "### Component-level testing\n",
    "- Test each node individually with controlled inputs to validate its specific functionality\n",
    "- Measure processing time for each component to identify bottlenecks\n",
    "- Evaluate embedding model performance using standard NLP metrics\n",
    "\n",
    "### End-to-end evaluation\n",
    "- Conduct user acceptance testing with real-world queries\n",
    "- Gather feedback on response quality, relevance, and usefulness\n",
    "- Use A/B testing to compare different workflow configurations\n",
    "\n",
    "## Continuous improvement\n",
    "- Implement logging to track performance metrics over time\n",
    "- Establish baseline performance and set improvement targets\n",
    "- Regularly review and refine the workflow based on performance data\n",
    "- Consider creating a feedback mechanism for users to rate response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414ca12",
   "metadata": {},
   "source": [
    "# -- Bottlenecks of the current implementation --\n",
    "\n",
    "## Performance limitations\n",
    "- **Memory consumption**: Loading multiple document embeddings simultaneously creates significant RAM requirements\n",
    "- **Processing latency**: Complex queries with deep analysis can take several seconds to complete\n",
    "- **Local model constraints**: Ollama's local deployment trades convenience for reduced inference capabilities compared to cloud models\n",
    "\n",
    "## Architectural constraints\n",
    "- **Sequential processing**: The workflow executes nodes sequentially rather than leveraging parallel processing\n",
    "- **Limited error handling**: Error management primarily focuses on content accuracy rather than technical failures\n",
    "- **Fixed retrieval strategies**: Predefined retrieval methods may not adapt well to highly specialized queries\n",
    "\n",
    "## Scaling challenges\n",
    "- **Document volume limitations**: Performance degrades with very large document collections\n",
    "- **Cold-start efficiency**: Initial setup requires full processing of all documents before first query\n",
    "\n",
    "## Future optimization opportunities\n",
    "- Add caching mechanisms for frequently accessed documents and embeddings\n",
    "- Introduce parallel processing for independent workflow nodes\n",
    "- Develop more sophisticated retrieval strategies with hybrid search capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
